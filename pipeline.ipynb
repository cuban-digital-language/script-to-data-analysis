{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dany/.local/share/virtualenvs/cuba-digital-lang-9Kd57kqM/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CubaDebate': 63452, 'Facebook': 287, 'Twitter': 109941, 'Telegram': 156440}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from submodulos.tokenizer.custom_tokenizer import SpacyCustomTokenizer\n",
    "from submodulos.data import get_all_text_with_details, fb\n",
    "from tools import get_progressbar\n",
    "from datetime import datetime\n",
    "import os\n",
    "from word_db import WordDB\n",
    "\n",
    "word_db = WordDB(localhost='localhost', port=6379, db=2)\n",
    "corpus = fb.get_details_text(f'{os.getcwd()}/submodulos/data/')\n",
    "name = 'facebook'\n",
    "\n",
    "\n",
    "texts = get_all_text_with_details(f'{os.getcwd()}/submodulos/data/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    date_distribution = {}\n",
    "    date_size = {}\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def save(self):\n",
    "        result = '{\\n'\n",
    "        result += f'\"date_distribution\": {str(self.date_distribution)},\\n'\n",
    "        result += f'\"date_size\": {str(self.date_size)},\\n'\n",
    "        result += '}'\n",
    "\n",
    "        with open(f'results/{self.name}.json', 'w+') as f:\n",
    "            f.write(result)\n",
    "            f.close()\n",
    "\n",
    "\n",
    "def print_best_word(list_):\n",
    "    for i in range(20):\n",
    "        print(list_[i], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = SpacyCustomTokenizer()\n",
    "bar = get_progressbar(len(corpus), f' from {len(corpus)} text, tokenizer and tag with date ')\n",
    "bar.start()\n",
    "token_list = []\n",
    "for i, data in enumerate(corpus):\n",
    "    t, d, n = data\n",
    "    for token in nlp(t):\n",
    "        token.data = d\n",
    "        token.source = n\n",
    "        token_list.append(token)\n",
    "    bar.update(i + 1)\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disjoin(_list, predicate, name = 'any feature'):\n",
    "    a, b = [], []\n",
    "    for item in _list:\n",
    "        if predicate(item): a.append(item)\n",
    "        else: b.append(item)\n",
    "\n",
    "    print(name, len(a), len(b))\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji 620 14667\n",
      "hashtag 126 14541\n",
      "user_tag 4 14537\n",
      "symbol 1870 12667\n",
      "space 2305 10362\n",
      "url 34 10328\n"
     ]
    }
   ],
   "source": [
    "emojis, tokens = disjoin(token_list, lambda x : x.is_emoji(), 'emoji')\n",
    "hashtags, tokens = disjoin(tokens, lambda x: x.is_hashtag(), 'hashtag') \n",
    "user_tags, tokens = disjoin(tokens, lambda x: x.is_user_tag(), 'user_tag') \n",
    "symbol, tokens = disjoin(tokens, lambda x: x.is_symbol, 'symbol')\n",
    "space, tokens = disjoin(tokens, lambda x: x.space() or len(x.text) == 0, 'space')\n",
    "urls, tokens = disjoin(tokens, lambda x: x.is_url(), 'url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "any feature 93 10235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ô∏è'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown, _ = disjoin(tokens, lambda x: x.unknown())\n",
    "\n",
    "# for token in unknown:\n",
    "#     print(token.text, end=' |')\n",
    "\n",
    "unknown[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## numbers\n",
    "## @\n",
    "## symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "any feature 9749 579\n"
     ]
    }
   ],
   "source": [
    "natural, _ = disjoin(tokens, lambda x: x.natural_word())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[######################################################] dictionary search 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574 6758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "error = []\n",
    "correct = []\n",
    "\n",
    "bar = get_progressbar(len(natural), ' dictionary search ')\n",
    "bar.start()\n",
    "for i, token in enumerate(natural):\n",
    "    obj = word_db.get(token.text)\n",
    "    if obj is None or obj == 'None' or len(obj) == 0: continue\n",
    "    if len(obj['rae']) == 1: error.append(token)\n",
    "    else: correct.append((token, obj['rae']['articles']))\n",
    "    bar.update(i + 1)\n",
    "bar.finish()\n",
    "\n",
    "print(len(error), len(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one def 4698 2060\n"
     ]
    }
   ],
   "source": [
    "one_def, mult_def = disjoin(correct, lambda x: len(x[1]) == 1, 'one def')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get etymology\n",
    "# get teclado error\n",
    "# get neutral error \n",
    "# get english word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorted_by_dict(d):\n",
    "    result = [(key, d[key] )for key in d]\n",
    "    result.sort(key= lambda x: x[1], reverse= True)\n",
    "    return result\n",
    "\n",
    "def count_tokens (tokens, predicate = lambda x: True, name = ' count any feature '):\n",
    "    bar = get_progressbar(len(tokens), name)\n",
    "\n",
    "    result = {}\n",
    "    bar.start()\n",
    "    for i, t in enumerate(tokens):\n",
    "        if not predicate(t): continue\n",
    "        try: result[t.text.lower()] += 1\n",
    "        except KeyError: \n",
    "            result[t.text.lower()] = 1\n",
    "            \n",
    "        bar.update(i + 1)\n",
    "    bar.finish()\n",
    "\n",
    "    return result, sorted_by_dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[######################################################] count any feature 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('‚Äº', 52) ('üëá', 29) ('üö®', 27) ('‚ù§', 25) ('üê¨', 20) ('üëå', 19) ('üõë', 15) ('üê¥', 15) ('üëâ', 14) ('üì≤', 13) ('üèª', 13) ('üëà', 12) ('üéâ', 11) ('üöå', 11) ('üôå', 9) ('üòâ', 8) ('ü§£', 8) ('üì±', 7) ('‚úÖ', 7) ('üíØ', 7) "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d, count = count_tokens(emojis)\n",
    "\n",
    "print_best_word(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Time Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density_time(tokens, predicate = lambda x: True, name= ' count any feature '):\n",
    "    bar = get_progressbar(len(tokens), name)\n",
    "\n",
    "    result = {}\n",
    "    bar.start()\n",
    "    for i, t in enumerate(tokens):\n",
    "        if not predicate(t): continue\n",
    "        date = str(datetime.strptime(t.data.split(' ')[0], '%Y-%m-%d'))\n",
    "        try: result[date][t.text.lower()] += 1\n",
    "        except KeyError: \n",
    "            try: result[date][t.text.lower()] = 1\n",
    "            except KeyError: \n",
    "                result[date] = {}\n",
    "                result[date][t.text.lower()] = 1\n",
    "            \n",
    "        bar.update(i + 1)\n",
    "    bar.finish()\n",
    "\n",
    "    return result\n",
    "\n",
    "def density_text_by_time(texts, name= ' count any feature '):\n",
    "    bar = get_progressbar(len(texts), name)\n",
    "\n",
    "    result = {}\n",
    "    bar.start()\n",
    "    for i, t in enumerate(texts):\n",
    "        t, d, _ = t\n",
    "        date = str(datetime.strptime(d.split(' ')[0], '%Y-%m-%d'))\n",
    "        try: result[date] += 1\n",
    "        except KeyError: \n",
    "            result[date] = 1\n",
    "            \n",
    "        bar.update(i + 1)\n",
    "    bar.finish()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[######################################################] count any feature 100%\n",
      "[######################################################] count any feature 100%\n"
     ]
    }
   ],
   "source": [
    "density_text = density_text_by_time(corpus)\n",
    "density_tokens = density_time(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuba-digital-lang",
   "language": "python",
   "name": "cuba-digital-lang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
